# MLOps Demo

A simple machine learning pipeline demo project.

<br/>

## Project Structure

```
mlops-demo/
├── data/
│   └── user_activity.csv       # Original CSV data
│   └── X.csv / y.csv          # Preprocessed input/target
│   └── scaler.pkl             # StandardScaler object for data scaling
├── model/
│   └── model.pkl              # Trained RandomForestClassifier model
├── preprocess.py              # Preprocessing script
├── train_model.py             # Training and saving script
├── predict_api.py             # FastAPI-based inference server
├── requirements.txt           # Dependencies
└── Dockerfile                 # Docker configuration
```

<br/>

## File Descriptions

### model.pkl
- File containing the saved RandomForestClassifier model
- Configured with n_estimators=100, random_state=42
- Saved using joblib.dump()
- Generated by running train_model.py in actual use

### scaler.pkl
- File containing the saved StandardScaler object
- Stores scaling parameters used in data preprocessing
- Saved using joblib.dump()
- Generated by running preprocess.py in actual use

<br/>

## Installation and Execution

<br/>

### 1. Local Environment Setup

1. Create and activate virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```
Result: The `(venv)` prompt appears, indicating the virtual environment is activated.

2. Install dependencies:
```bash
pip3 install -r requirements.txt
```
Result: Required packages are installed, showing installation progress.
```
Collecting numpy>=1.26.0
Collecting pandas>=2.1.0
Collecting scikit-learn>=1.3.2
...
Successfully installed numpy-1.26.0 pandas-2.1.0 scikit-learn-1.3.2 ...
```

3. Preprocess data:
```bash
python3 preprocess.py
```
Result: Data preprocessing completes and the following files are generated:
```
data/X.csv  # Preprocessed input data
data/y.csv  # Preprocessed target data
data/scaler.pkl  # Scaling parameters
```

4. Train model:
```bash
python3 train_model.py
```
Result: Model training completes and the following file is generated:
```
model/model.pkl  # Trained model
```
Training results are displayed:
```
Model Accuracy: 0.XX
Model saved to model/model.pkl
```

5. Run API server:
```bash
python3 predict_api.py
```
Result: FastAPI server starts and the following message is displayed:
```
INFO:     Started server process [xxxxx]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

6. Deactivate virtual environment:
```bash
deactivate
```
Result: The `(venv)` prompt disappears, indicating the virtual environment is deactivated.

<br/>

### 2. Docker Setup

1. Build Docker image:
```bash
docker build -t mlops-demo .
```
Result: Docker image is built and the following message is displayed:
```
Sending build context to Docker daemon  XX.XXMB
Step 1/4 : FROM python:3.13-slim
...
Step 5/7 : RUN python preprocess.py && python train_model.py
Preprocessing completed:
- Training data: XX samples
- Test data: XX samples
- Features: ['session_duration', 'page_views', 'clicks', 'scroll_depth', 'time_on_site']
Model Accuracy: 0.XX
Model saved to model/model.pkl
...
Successfully built xxxxxxxxxxxx
Successfully tagged mlops-demo:latest
```

2. Run Docker container:
```bash
docker run -p 8000:8000 mlops-demo
```
Result: Docker container starts and FastAPI server runs:
```
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

**Note**: Data preprocessing and model training are automatically executed during Docker build, so you don't need to run `preprocess.py` and `train_model.py` separately.

<br/>

### 3. MLflow Experiment Tracking and UI

1. Run MLflow experiment tracking example:
```bash
python3 mlflow_test.py
```
- Scikit-Learn experiments are automatically tracked and results are saved in the `mlruns/` folder.

2. Launch MLflow UI:
```bash
mlflow ui
```
- Open [http://localhost:5000](http://localhost:5000) in your browser to visually inspect experiment results.

**Tips**
- Make sure `mlflow` is included in requirements.txt.
- Add `mlruns/` to `.gitignore` to avoid committing experiment logs.
- In Docker, use port mapping (`-p 5050:5050`) to access the UI externally.

**Example file**: `mlflow_test.py`

#### Run predict API and MLflow UI containers with shared mlruns folder

1. Build predict API server image:
```bash
docker build -f Dockerfile.predictapi -t mlops-predictapi .
```
2. Build MLflow UI image:
```bash
docker build -f Dockerfile.mlflowui -t mlops-mlflowui .
```

3. Run predict API server container (share mlruns):
```bash
docker run -p 8000:8000 -v $(pwd)/mlruns:/app/mlruns mlops-predictapi
```
4. Run MLflow UI container (share mlruns):
```bash
docker run -p 5050:5050 -v $(pwd)/mlruns:/app/mlruns mlops-mlflowui
```
5. Open [http://localhost:5050](http://localhost:5050) in your browser

- When both containers share the same mlruns folder, MLflow runs created by the predict API are immediately visible in the UI.

<br/>

## API Usage

<br/>

### 1. Single prediction (CSV file)
```bash
curl -X POST "http://localhost:8000/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@data/test.csv"
```
Result: Returns prediction results in JSON format:
```json
{
  "predictions": [0, 1, 0],
  "probabilities": [0.2, 0.8, 0.3]
}
```

**Note**: If using Docker, create test.csv first:
```bash
# Create test data file
echo "session_duration,page_views,clicks,scroll_depth,time_on_site
130,6,9,80,190
50,2,3,35,65
170,7,11,85,210" > test.csv

# Then use the file
curl -X POST "http://localhost:8000/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@test.csv"
```

**Important**: If test.csv is in the data/ folder, you need to either:
1. Change to the data directory first:
```bash
cd data/
curl -X POST "http://localhost:8000/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@test.csv"
```

2. Or use the full path from the project root:
```bash
curl -X POST "http://localhost:8000/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@data/test.csv"
```

<br/>

### 2. Batch prediction (JSON)
```bash
curl -X POST "http://localhost:8000/predict_batch" -H "accept: application/json" -H "Content-Type: application/json" -d '[{"session_duration": 130, "page_views": 6, "clicks": 9, "scroll_depth": 80, "time_on_site": 190}]'
```
Result: Returns prediction results in JSON format:
```json
{
  "predictions": [1],
  "probabilities": [0.75]
}
```

<br/>

### 3. Using Python requests library
```python
import requests
import pandas as pd

# Predict with CSV file
with open('test.csv', 'rb') as f:
    files = {'file': f}
    response = requests.post('http://localhost:8000/predict', files=files)
    print(response.json())

# Predict with JSON data
data = [{"session_duration": 130, "page_views": 6, "clicks": 9, "scroll_depth": 80, "time_on_site": 190}]
response = requests.post('http://localhost:8000/predict_batch', json=data)
print(response.json())
```

**Example file**: `examples/python_example.py`

<br/>

### 4. Using JavaScript/Fetch API
```javascript
// Predict with CSV file
const formData = new FormData();
formData.append('file', fileInput.files[0]);

fetch('http://localhost:8000/predict', {
    method: 'POST',
    body: formData
})
.then(response => response.json())
.then(data => console.log(data));

// Predict with JSON data
const data = [{"session_duration": 130, "page_views": 6, "clicks": 9, "scroll_depth": 80, "time_on_site": 190}];

fetch('http://localhost:8000/predict_batch', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
    },
    body: JSON.stringify(data)
})
.then(response => response.json())
.then(data => console.log(data));
```

**Example files**: `examples/web_example.html`, `examples/web_example.js`

<br/>

### 5. Using Postman
1. **CSV file prediction**:
   - Method: POST
   - URL: `http://localhost:8000/predict`
   - Body: form-data
   - Key: `file` (File type)
   - Value: Select test.csv file

2. **JSON prediction**:
   - Method: POST
   - URL: `http://localhost:8000/predict_batch`
   - Body: raw (JSON)
   - Content: `[{"session_duration": 130, "page_views": 6, "clicks": 9, "scroll_depth": 80, "time_on_site": 190}]`

**Example files**: `examples/postman_collection.json`

<br/>

### 6. Using HTTPie
```bash
# CSV file prediction
http -f POST localhost:8000/predict file@test.csv

# JSON prediction
http POST localhost:8000/predict_batch session_duration:=130 page_views:=6 clicks:=9 scroll_depth:=80 time_on_site:=190
```

**Example file**: `examples/httpie_example.sh`

### 7. Quick test with inline data (Docker friendly)
```bash
# Create test file and predict in one command
echo "session_duration,page_views,clicks,scroll_depth,time_on_site
130,6,9,80,190" > test.csv && curl -X POST "http://localhost:8000/predict" -H "accept: application/json" -H "Content-Type: multipart/form-data" -F "file=@test.csv"
```

**Example file**: `examples/quick_test.sh`

<br/>

## API Documentation

FastAPI auto-generated documentation is available at:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

<br/>

## Examples

The `examples/` directory contains various ways to test and use the API:

### Quick Start with Examples
```bash
# Install example dependencies
pip install -r examples/requirements.txt

# Run Python example
python examples/python_example.py

# Run quick test
./examples/quick_test.sh

# Open web interface
open examples/web_example.html
```

### Available Examples
- **Python**: Complete script with error handling (`python_example.py`)
- **Web Interface**: Interactive HTML page (`web_example.html`)
- **Postman**: Import collection (`postman_collection.json`)
- **Shell Scripts**: Various command-line examples
- **Documentation**: See `examples/README.md` for detailed usage

### Example Dependencies
```bash
# Required for Python examples
pip install requests

# Optional for HTTPie examples
pip install httpie
```

## Container Cleanup

To stop and remove all Docker containers and images (use with caution):

```bash
# Stop all running containers
docker ps -aq | xargs docker stop

# Remove all containers
docker ps -aq | xargs docker rm

# Remove all images
docker images -aq | xargs docker rmi
```

> **Note:** These commands will stop and delete all containers and images on your system. Use them only if you are sure you want to clean up everything.



